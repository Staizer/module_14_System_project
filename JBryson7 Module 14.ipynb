{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15066c37",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The intent of this program is to pull data information from a csv:\n",
    "Process Task, Process Category, Process, Inputs1, Inputs2, Inputs3, Inputs4, Inputs5, Inputs6, Inputs7, Inputs8, T&T1, T&T2, T&T3, T&T4, T&T5, T&T6, T&T7, T&T8, T&T9, Outputs1, Outputs2, Outputs3, Outputs4, Outputs5, Outputs6, Outputs7, Outputs8, Outputs9, Outputs10\n",
    "For example:\n",
    "Monitoring and Controlling, Quality, Executing, Project management Plan, Project documents, Organizational process assets, , , , , , Data gathering, Data analysis, Decision making, Data representation, Audits, Design for X, Problem solving, Quality improvement methods, , Quality reports, Test and evaluation documents, Change requests, Project management plan updates, Project documents updates , , , , ,\n",
    "\n",
    "And create an association network from process to process category, to process task, to input and output.\n",
    "\n",
    "After the inputs and outputs are associated with the process, another csv with sample filenames will be imported and the file names will be compared to the inputs/outputs.\n",
    "\n",
    "There are many procedural problems with this effort. More time is needed to create a true \"synonym list\" for each input/output. Simply going off of true grammatical synonyms is not accurate enough. An industry standard for what exactly are Organizational process assets, for example, is needed. Additionally, getting ahold of real files to evaluate there names was impossible in the time that I had available, so I ended up generating fake names. Since there is little repetition between filenames, the AI, if it were trained, would be heavily overfit.\n",
    "\n",
    "If I had more time, I would attempt to put together some sort of probability function that determines the number of input/output references for each process. If Project Documents occurs more in one process than another, then the probability that a file that is called a project document would be associated with that process over a less likely one. The AI would then train to min/max these relations.\n",
    "\n",
    "A link to the github repository and docker are below:\n",
    "\n",
    "Github:\n",
    "https://github.com/Staizer/module_14_System_project.git\n",
    "\n",
    "Docker:\n",
    "https://hub.docker.com/r/staizer/module14_system_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b6c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "\n",
    "import nltk #intended to do some AI computation on text\n",
    "from nltk.stem.wordnet import WordNetLemmatizer as WNL #used to develop the lemmas of file names and input/output types\n",
    "from nltk.corpus import wordnet as wn #used to find synonyms for file names and input/outputs\n",
    "import pandas as pd #unused for the moment\n",
    "from wordsegment import load, segment #since filenames often come as a single text string of words (I.E. useragreement), \n",
    "# I needed a way to segment these strings out into possible words. Still does incorrect work though. \n",
    "#(I.E. studentinfo segments to student, in, f, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate several classes to create objects for correlation\n",
    "class Category: #class intended to store category name and associated tasks\n",
    "    def __init__(self, category):\n",
    "        self.category = category\n",
    "        self.tasks = []\n",
    "\n",
    "    def __str__(self): # prints the number and suit of the card when called\n",
    "        return f\"{self.category}\"\n",
    "\n",
    "class Task: #class intended to store task name and associated inputs and outputs\n",
    "    def __init__(self, task):\n",
    "        self.task = task\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "\n",
    "    def __str__(self): # prints the number and suit of the card when called\n",
    "        return f\"{self.task}\"\n",
    "\n",
    "class Word: #class intended to store the word and it's synonyms\n",
    "    def __init__(self,word):\n",
    "        self.word = word\n",
    "        self.synonyms = []\n",
    "\n",
    "class Process: #class intended to store the process name, and its inputs, outputs, tasks, and categories\n",
    "    def __init__(self,process):\n",
    "        self.process = process\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.categories = []\n",
    "        self.tasks = []\n",
    "        \n",
    "    def __str__(self): # prints the number and suit of the card when called\n",
    "        return f\"{self.process}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d32b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#various functions that will be used to separate and manipulate the data sets\n",
    "def open_file(file): #used to take in csv and make a single list from each line\n",
    "    line_list = []\n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            else:\n",
    "                line = line.strip(\"\\n\")\n",
    "                line_list.append(line.split(','))\n",
    "    # print(line_list)\n",
    "    return line_list\n",
    "#O(n) where n is the number of lines in the file\n",
    "\n",
    "#finds the process column and stores all processes in a list, then assigns each process name to a process object\n",
    "def process(lines): \n",
    "    processes = []\n",
    "    process_list = []\n",
    "    for i in lines:\n",
    "        processes.append(i[2])\n",
    "    \n",
    "    processes = list(set(processes))\n",
    "    \n",
    "    for i in processes:\n",
    "        P = Process(i)\n",
    "        process_list.append(P)\n",
    "    return process_list\n",
    "#O(lines)\n",
    "\n",
    "\n",
    "#finds the category column and stores all categories in a list, then assigns each category name to a category object\n",
    "def category(lines):\n",
    "    categories = []\n",
    "    category_list = []\n",
    "    for i in lines:\n",
    "        categories.append(i[1])\n",
    " \n",
    "    categories = list(set(categories))\n",
    "    for i in categories:\n",
    "        C = Category(i)\n",
    "        category_list.append(C)\n",
    "    return category_list\n",
    "#O(lines)\n",
    "\n",
    "#finds the task column and stores all tasks in a list, then assigns each task name to a task object\n",
    "def tasks(lines):\n",
    "    tasks = []\n",
    "    task_list = []\n",
    "    for i in lines:\n",
    "        tasks.append(i[0])\n",
    "    for i in tasks:\n",
    "        T = Task(i)\n",
    "        task_list.append(T)\n",
    "    task_list = list(set(task_list))\n",
    "    return task_list\n",
    "#O(lines)\n",
    "\n",
    "#finds the input coluumns and stores all inputs in a list after cleaning it\n",
    "def inputs(lines):\n",
    "    inputs = []\n",
    "    for i in lines:\n",
    "        inputs.append([i[3],i[4],i[5],i[6],i[7],i[8],i[9],i[10]])\n",
    "    for i in inputs:\n",
    "        for j in range(len(i)-1,0,-1):\n",
    "            if i[j] == '':\n",
    "                i.pop(j)\n",
    "            else:\n",
    "                continue\n",
    "    return inputs\n",
    "#O(lines^2)\n",
    "\n",
    "#finds the output coluumns and stores all outputs in a list after cleaning it\n",
    "def outputs(lines):\n",
    "    outputs = []\n",
    "    for i in lines:\n",
    "        outputs.append([i[20],i[21],i[22],i[23],i[24],i[25],i[26],i[27],i[28],i[29]])\n",
    "    for i in outputs:\n",
    "        for j in range(len(i)-1,0,-1):\n",
    "            if i[j] == '':\n",
    "                i.pop(j)\n",
    "            else:\n",
    "                continue\n",
    "    return outputs\n",
    "#O(lines^2)\n",
    "\n",
    "#finds the lemmas of a word\n",
    "def lemmize(words):\n",
    "    word_list = 0\n",
    "    lemmize = []\n",
    "    for i in range(len(words)):\n",
    "        if word_list == 0:\n",
    "            word_list = words[i]\n",
    "        else:\n",
    "            word_list = word_list + ' ' + words[i]\n",
    "    wtoken = nltk.word_tokenize(word_list)\n",
    "    for f in wtoken:\n",
    "        lemmize.append(WNL().lemmatize(f, 'v'))\n",
    "    return lemmize\n",
    "#O(words)\n",
    "\n",
    "#finds the synonym of each lemma\n",
    "def synonyms(word):\n",
    "    # print(word)\n",
    "    synonyms = wn.synsets(word)\n",
    "    return synonyms\n",
    "#O(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14672165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main part of program, creates various lists to be used later\n",
    "file = 'Project Management Inputs Outputs and Tasks.csv'\n",
    "train = 'file_names.csv'\n",
    "opened = open_file(file) #has 49 lines\n",
    "open_train = open_file(train) #train has 13000 files\n",
    "training = []\n",
    "for i in open_train:\n",
    "    for j in i:\n",
    "        x = j.split('_',5)\n",
    "        x = x[-1]\n",
    "        x = x.split('.')\n",
    "        x = x[0]\n",
    "        training.append(x)\n",
    "#O(13000^2)\n",
    "training = sorted(training)\n",
    "\n",
    "lines = []\n",
    "for i in opened:\n",
    "    line = []\n",
    "    for j in i:\n",
    "        if j =='\\n':\n",
    "            continue\n",
    "        else:\n",
    "            line.append(j)\n",
    "    lines.append(line)\n",
    "#O(49^2)\n",
    "processes = process(lines)\n",
    "categories = category(lines)\n",
    "tasks = tasks(lines)\n",
    "inputs = inputs(lines)\n",
    "outputs = outputs(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37269472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The beginning of my troubles, far too many for loops for that amount of work that I need them to do. The intent here is to \n",
    "#create the nodal connections between processes, process categories, process tasks, and inputs/outputs\n",
    "for b,i in enumerate(lines):\n",
    "    for j in categories:\n",
    "        if i[1] == j.category:\n",
    "            j.tasks.append(i[0])\n",
    "    for a,j in enumerate(tasks):\n",
    "        if i[0] == j.task:\n",
    "            j.inputs = lemmize(inputs[a])\n",
    "            j.outputs = lemmize(outputs[a])\n",
    "    for j in processes:\n",
    "        if i[2] == j.process:\n",
    "            for k in categories:\n",
    "                if i[1] == k.category:\n",
    "                    j.categories.append(k)\n",
    "            j.tasks.append(i[0])\n",
    "            j.inputs.append(lemmize(inputs[b]))\n",
    "            j.outputs.append(lemmize(outputs[b]))\n",
    "#O(lines*(category+task+process))            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe5e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my attempt at generating a list of synonyms for both files. This is where the program finally breaks completely.\n",
    "synonym = []\n",
    "\n",
    "for line in inputs:    \n",
    "    for word in line:\n",
    "        synonym.append(synonyms(word))\n",
    "#O(inputs^2)            \n",
    "for line in outputs:\n",
    "    for word in line:\n",
    "        synonym.append(synonyms(word))\n",
    "#O(outputs^2)            \n",
    "train_syn = []\n",
    "for line in training:\n",
    "    for word in line:\n",
    "        train_syn.append(synonyms(word))\n",
    "#O(training^2)                \n",
    "similarities = []    \n",
    "s_one = []\n",
    "s_two = []\n",
    "for syn in synonym:\n",
    "    for s in syn:\n",
    "        s_one.append(s)\n",
    "#O(synonyms)\n",
    "for train in train_syn:\n",
    "    for t in train:\n",
    "        s_two.append(t)\n",
    "#O(training synonyms)\n",
    "for s in s_one:\n",
    "    for t in s_two:\n",
    "        similarities.append(s.wup_similarity(t))\n",
    "print(similarities) \n",
    "#O(n*m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671684a3",
   "metadata": {},
   "source": [
    "If I had more time, I would attempt to re-write this code, but I fear that any change I make will only make matters worse and won't solve my fundeaental problems. I need better data, and a more refined list of inputs/outputs to train an AI to identify."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
